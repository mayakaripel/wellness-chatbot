{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7094871,"sourceType":"datasetVersion","datasetId":4088849},{"sourceId":28785,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":8318,"modelId":3301}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"a9f6895b-c543-47be-815e-fbc192b10c54","_cell_guid":"505a06d9-1454-4081-bd9b-5a6628b998ea","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Install Required Libraries","metadata":{"_uuid":"13ee23e0-380b-4daf-a42c-319c05eeb91b","_cell_guid":"02f15aa5-8e81-4d62-9352-372fd3f6ae0c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install torchvision\n!pip install faiss-gpu\n!pip install -q -U transformers datasets bitsandbytes peft evaluate sentence-transformers\n!pip install --upgrade transformers\n!pip install ipywidgets\n!pip install --upgrade ipywidgets","metadata":{"_uuid":"d63577f8-4d68-410b-907d-37a6c1b263b7","_cell_guid":"9da44f95-5d3a-49ee-b246-2e6289eb695a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Import Libraries and Configure Environment","metadata":{"_uuid":"63d68c1c-96cc-4021-84c2-b22b56f3e78a","_cell_guid":"2cc9e655-a214-4131-a5f5-406432afeb4e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\nfrom sentence_transformers import SentenceTransformer, util\nfrom accelerate import Accelerator\nimport faiss\nfrom transformers import pipeline\nimport evaluate\nimport shutil\n\n# Disable W&B logging\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Clear the cache\ncache_dir = os.path.expanduser(\"~/.cache/huggingface\")\nshutil.rmtree(cache_dir, ignore_errors=True)\n\n# Clear GPU memory\ntorch.cuda.empty_cache()","metadata":{"_uuid":"3659a515-7f32-418a-84d9-cef2cb75ce29","_cell_guid":"68861410-368f-4b3a-823e-322d48e94adb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load Model and Tokenizer","metadata":{"_uuid":"599fdf3f-d795-45e8-9669-7b9547015730","_cell_guid":"a80afdd8-59c6-48b0-8be4-d4d87722183d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Verify the model name\nmodel_name = \"/kaggle/input/gemma/transformers/2b-it/3\"  # Use the gemma model\n\n# Load the model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(model_name, local_files_only=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n\nprint(\"Model and tokenizer loaded successfully!\")\n\n# Inspect the model architecture to find the correct target modules\nfor name, module in model.named_modules():\n    print(name)","metadata":{"_uuid":"ce7d62b4-094e-4469-9329-23aae73947e1","_cell_guid":"28d20853-8813-4a6c-a054-d162058079d2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Configure LoRA","metadata":{"_uuid":"bb8c1157-c860-47fe-a2f8-6f03690d7979","_cell_guid":"0fe30643-3574-45f3-9ab7-ecbf8664ecae","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Update the target modules based on the inspection\ntarget_modules = [\n    \"model.layers.0.self_attn.q_proj\",\n    \"model.layers.0.self_attn.k_proj\",\n    \"model.layers.0.self_attn.v_proj\",\n    \"model.layers.1.self_attn.q_proj\",\n    \"model.layers.1.self_attn.k_proj\",\n    \"model.layers.1.self_attn.v_proj\",\n    # Add more layers as needed\n]\n\n# LoRA Configuration\npeft_config = LoraConfig(r=8, lora_alpha=32, lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=target_modules)\nmodel = get_peft_model(model, peft_config)","metadata":{"_uuid":"ebc64838-438c-4bcf-b4eb-6aad68621989","_cell_guid":"6b8e0ff1-66bc-4a7b-aaa3-dba50354178b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load and Preprocess Dataset","metadata":{"_uuid":"2185fc1b-76ba-4fac-ba47-1c4455eef7fd","_cell_guid":"c4a6238b-d153-4e4c-b480-4158025f75f9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load Dataset\ndataset = load_dataset(\"Amod/mental_health_counseling_conversations\")\n\n# Preprocess Data\ndef preprocess_function(examples):\n    for i in range(len(examples['Response'])):\n        examples['Response'][i] = \"<user> \" + examples['Response'][i] + \" <counselor> \"\n    return examples\n\ndataset = dataset.map(preprocess_function, batched=True)\n\n# Split dataset\ntrain_test_split = dataset['train'].train_test_split(test_size=0.1, seed=42)\ntrain_data = train_test_split['train']\nval_data = train_test_split['test']\n\n# Check for empty strings and filter by token length\nval_data = val_data.filter(lambda example: len(tokenizer(example['Response'], truncation=True, max_length=512)['input_ids']) > 1)","metadata":{"_uuid":"f670d50e-b7ce-4ef7-b826-af5e7c1ce24f","_cell_guid":"3c9e3962-66bf-42c9-875d-ac162ad4b8f1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Prepare Embeddings with SentenceTransformer and FAISS","metadata":{"_uuid":"64fff05d-deda-447e-ba0a-7df8ce5c0eac","_cell_guid":"caf1ed38-ac1d-48df-aca7-0216e9871ef6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Initialize SentenceTransformer model\nsentence_model = SentenceTransformer('all-MiniLM-L6-v2')  # Experiment with different models\n\n# Prepare Data Embeddings with FAISS in Batches\nbatch_size = 128  # Adjust as needed\ntrain_embeddings = []\nfor i in range(0, len(train_data['Response']), batch_size):\n    batch = train_data['Response'][i:i + batch_size]\n    train_embeddings.extend(sentence_model.encode(batch))\ntrain_embeddings = np.array(train_embeddings)\nindex = faiss.IndexFlatL2(train_embeddings.shape[1])\nindex.add(train_embeddings)","metadata":{"_uuid":"48821cc8-04ed-4d1a-b715-8b9735edf576","_cell_guid":"f3c8886c-e867-4302-a161-44c91d6f9400","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define Chatbot Response and Reward Functions","metadata":{"_uuid":"8839788c-786d-4275-b1a8-1676a85c234e","_cell_guid":"5fd46507-8611-4904-af2c-1e2dc6485046","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Semantic Search with FAISS and Keyword-Based Search\ndef semantic_chatbot_response(user_input):\n    user_embedding = sentence_model.encode(user_input)\n    D, I = index.search(np.expand_dims(user_embedding, axis=0), k=1)  # Top 1 result\n    most_similar_response = train_data['Response'][I[0][0]]\n\n    # Keyword-based search\n    keyword_responses = [response for response in train_data['Response'] if any(keyword in response for keyword in user_input.split())]\n    if keyword_responses:\n        most_similar_response = keyword_responses[0]\n\n    if D[0][0] > 0.7:  # Adjust threshold for similarity\n        print(\"(Generating response with language model...)\")\n        input_ids = tokenizer(user_input, return_tensors=\"pt\").input_ids.to(model.device)\n        with torch.no_grad():\n            output = model.generate(\n                input_ids,\n                max_new_tokens=50,\n                do_sample=False,\n                top_k=50,\n                temperature=0.7,\n                eos_token_id=tokenizer.eos_token_id\n            )\n        most_similar_response = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    return most_similar_response\n\ndef get_reward(response, user_input):\n    \"\"\"\n    Calculates a reward for the chatbot's response based on various criteria.\n\n    Args:\n      response: The chatbot's generated response.\n      user_input: The user's input to the chatbot.\n\n    Returns:\n      A float representing the overall reward for the response.\n    \"\"\"\n    # 1. Distinctness from Input\n    response_embedding = sentence_model.encode(response)\n    input_embedding = sentence_model.encode(user_input)\n    similarity = util.cos_sim(response_embedding, input_embedding)[0][0].item()\n    distinctness_score = 1.0 - similarity\n\n    # 2. Conciseness\n    conciseness_score = 1.0 / len(response.split())\n\n    # 3. Empathy/Helpfulness (using a sentiment analysis pipeline)\n    classifier = pipeline(\"sentiment-analysis\") \n    sentiment_result = classifier(response)\n    empathy_score = sentiment_result[0]['score']\n\n    # 4. Safety (using a toxicity detection pipeline)\n    toxicity_classifier = pipeline(\"toxicity-detection\")\n    toxicity_result = toxicity_classifier(response)\n    safety_score = 1.0 - toxicity_result[0]['score']\n\n    # Combine scores (adjust weights as needed)\n    overall_reward = (\n        distinctness_score * 0.3 + \n        conciseness_score * 0.2 + \n        empathy_score * 0.3 + \n        safety_score * 0.2\n    )  \n\n    return overall_reward","metadata":{"_uuid":"290541f9-a3bb-433a-8cb2-7688b139f563","_cell_guid":"3a2ae2a6-87c1-4959-a0db-9aae821cbd7c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train and Run Chatbot","metadata":{"_uuid":"08e22f74-4fda-4adb-b81b-0584c2c5ccb0","_cell_guid":"57ced4e0-499b-42c6-9f4e-7cfb1d9287e5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def rl_update(model, optimizer, user_input_ids, response_ids, reward):\n    model.train()\n    \n    max_length = max(len(ids[0]) for ids in [user_input_ids, response_ids])\n    \n    padded_user_input_ids = tokenizer.pad({'input_ids': user_input_ids}, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n    padded_response_ids = tokenizer.pad({'input_ids': response_ids}, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n    \n    outputs = model(padded_user_input_ids, labels=padded_response_ids)\n    loss = outputs.loss * reward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n# Chatbot Interaction\ndef run_chatbot():\n    print(\"Welcome to the Wellness Chatbot! Type 'exit' to end the conversation.\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == 'exit':\n            print(\"Chatbot: Take care! I'm here if you need to talk again.\")\n            break\n        # Format the input\n        formatted_input = f\"<user> {user_input} <counselor>\"\n        input_ids = tokenizer(formatted_input, return_tensors=\"pt\").input_ids.to(model.device)\n        with torch.no_grad():\n            output = model.generate(input_ids, max_new_tokens=50, eos_token_id=tokenizer.eos_token_id)\n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        # Extract the counselor's response\n        counselor_response = response.split(\"<counselor>\")[-1].strip()\n        print(f\"Chatbot: {counselor_response}\")\n\n# Start the chatbot\nrun_chatbot()","metadata":{"_uuid":"fe0fd190-a774-4983-afb3-e8d0ec262f63","_cell_guid":"4a830993-43da-4819-964d-baffce5fb5ca","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Calculate Perplexity","metadata":{"_uuid":"7e9d94cf-6e1c-4860-b1e1-8eec80328766","_cell_guid":"597f50c7-c561-466c-8da9-a68ad1964096","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def calculate_perplexity_iterative(model, tokenizer, dataset, batch_size=1):\n    model.eval()\n    total_perplexity = 0\n    num_batches = 0\n    with torch.no_grad():\n        for i in range(0, len(dataset), batch_size):\n            batch = dataset[i:i + batch_size]\n            inputs = tokenizer(batch['Response'], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n            inputs = inputs.to(model.device)\n            with torch.cuda.amp.autocast():\n                outputs = model(**inputs, labels=inputs['input_ids'])\n            log_prob = outputs.loss\n            perplexity = np.exp(log_prob.item())\n            total_perplexity += perplexity\n            num_batches += 1\n            print(f\"Batch {num_batches} perplexity: {perplexity}\")\n    avg_perplexity = total_perplexity / num_batches\n    print(f\"Average Perplexity: {avg_perplexity}\")\n\n# Test on val data\ncalculate_perplexity_iterative(model, tokenizer, val_data)","metadata":{"_uuid":"5dc0031c-db91-44d1-9ede-c9368f598128","_cell_guid":"b0e801e3-4bbd-4b2e-a503-0bde4b550949","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}